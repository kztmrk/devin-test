import json
import time
from typing import Any, Dict, Generator, List, Optional

from duckduckgo_search import DDGS
from openai import AzureOpenAI
from pydantic import BaseModel, Field

from ..base.base_agent import BaseAgent


class SearchDecision(BaseModel):
    """æ¤œç´¢ã®å¿…è¦æ€§ã‚’åˆ¤æ–­ã™ã‚‹ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«"""

    should_search: bool = Field(description="æ¤œç´¢ãŒå¿…è¦ã‹ã©ã†ã‹ã‚’ç¤ºã™ãƒ–ãƒ¼ãƒ«å€¤")
    reason: str = Field(description="åˆ¤æ–­ã®ç†ç”±")


class SearchQuery(BaseModel):
    """æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«"""

    query: str = Field(description="ç”Ÿæˆã•ã‚ŒãŸæ¤œç´¢ã‚¯ã‚¨ãƒª")
    keywords: list[str] = Field(description="æŠ½å‡ºã•ã‚ŒãŸä¸»è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", default_factory=list)


class QueryRefinement(BaseModel):
    """æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ã™ã‚‹ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«"""

    should_refine: bool = Field(description="ã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹ã©ã†ã‹")
    refined_query: str = Field(description="æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒªï¼ˆæœ€é©åŒ–ãŒä¸è¦ãªå ´åˆã¯ç©ºæ–‡å­—åˆ—ï¼‰")
    reason: str = Field(description="æœ€é©åŒ–ã®ç†ç”±ã¾ãŸã¯ä¸è¦ã¨åˆ¤æ–­ã—ãŸç†ç”±")


class DateExtraction(BaseModel):
    """æ—¥ä»˜æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«"""

    date_found: bool = Field(description="æ—¥ä»˜ãŒè¦‹ã¤ã‹ã£ãŸã‹ã©ã†ã‹")
    date: str = Field(description="æŠ½å‡ºã•ã‚ŒãŸæ—¥ä»˜æƒ…å ±ï¼ˆè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆã¯ç©ºæ–‡å­—åˆ—ï¼‰")
    format: str = Field(description="æ—¥ä»˜ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆä¾‹ï¼šYYYY/MM/DDï¼‰", default="")


class SourceClassification(BaseModel):
    """æƒ…å ±ã‚½ãƒ¼ã‚¹ã‚’åˆ†é¡ã™ã‚‹ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«"""

    source_type: str = Field(description="ã€Œä¸€æ¬¡æƒ…å ±ã€ã€ã€ŒäºŒæ¬¡æƒ…å ±ã€ã€ã¾ãŸã¯ã€Œä¸æ˜ã€")
    confidence: float = Field(description="åˆ†é¡ã®ç¢ºä¿¡åº¦ï¼ˆ0.0-1.0ï¼‰", ge=0.0, le=1.0)
    reason: str = Field(description="åˆ†é¡ã®ç†ç”±")


class SourceCitation(BaseModel):
    """æƒ…å ±ã‚½ãƒ¼ã‚¹ã®å¼•ç”¨æƒ…å ±ã‚’ç®¡ç†ã™ã‚‹ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«"""

    source_id: int = Field(description="å¼•ç”¨ã‚½ãƒ¼ã‚¹ã®è­˜åˆ¥ç•ªå·ï¼ˆä¾‹ï¼š[1], [2]ï¼‰")
    title: str = Field(description="å¼•ç”¨å…ƒã®ã‚¿ã‚¤ãƒˆãƒ«")
    url: str = Field(description="å¼•ç”¨å…ƒã®URL")
    date: Optional[str] = Field(description="å…¬é–‹æ—¥æ™‚ï¼ˆã‚ã‹ã‚Œã°ï¼‰", default=None)
    source_type: str = Field(description="æƒ…å ±ã‚½ãƒ¼ã‚¹ã®ç¨®é¡ï¼ˆä¸€æ¬¡æƒ…å ±/äºŒæ¬¡æƒ…å ±/ä¸æ˜ï¼‰", default="ä¸æ˜")
    excerpt: str = Field(description="å¼•ç”¨ã™ã‚‹æƒ…å ±ã®æŠœç²‹", default="")


class DuckDuckGoSearchAgent(BaseAgent):
    """
    An implementation of BaseAgent that uses DuckDuckGo search to enhance responses.
    This agent can search for external information to provide more accurate and up-to-date responses.
    """

    def initialize(self) -> None:
        """
        Initialize the Azure OpenAI client and DuckDuckGo search.
        """
        self.client = None
        if all(k in self.config for k in ["api_key", "api_version", "azure_endpoint"]):
            self.client = AzureOpenAI(
                api_key=self.config["api_key"],
                api_version=self.config["api_version"],
                azure_endpoint=self.config["azure_endpoint"],
            )

        if "deployment_name" not in self.config:
            self.config["deployment_name"] = "gpt-35-turbo"

        if "system_message" not in self.config:
            self.config[
                "system_message"
            ] = """ã‚ãªãŸã¯å¤–éƒ¨æƒ…å ±æ¤œç´¢æ©Ÿèƒ½ã‚’æŒã¤å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€å¿…è¦ã«å¿œã˜ã¦DuckDuckGoã§æ¤œç´¢ã‚’è¡Œã„ã€æœ€æ–°ã‹ã¤æ­£ç¢ºãªæƒ…å ±ã‚’æä¾›ã—ã¾ã™ã€‚

æƒ…å ±ã®ä¿¡é ¼æ€§ã‚’è©•ä¾¡ã™ã‚‹éš›ã¯ã€ä»¥ä¸‹ã®ç‚¹ã«æ³¨æ„ã—ã¦ãã ã•ã„ï¼š
1. ä¸€æ¬¡æƒ…å ±ï¼ˆå…¬å¼ç™ºè¡¨ã€åŸè‘—è«–æ–‡ãªã©ï¼‰ã¯äºŒæ¬¡æƒ…å ±ã‚ˆã‚Šã‚‚ä¿¡é ¼æ€§ãŒé«˜ã„å‚¾å‘ãŒã‚ã‚Šã¾ã™
2. æƒ…å ±ã®æ–°ã—ã•ï¼ˆå…¬é–‹æ—¥æ™‚ï¼‰ã‚’è€ƒæ…®ã—ã€æœ€æ–°ã®æƒ…å ±ã‚’å„ªå…ˆã—ã¦ãã ã•ã„
3. è¤‡æ•°ã®æƒ…å ±æºãŒä¸€è‡´ã™ã‚‹æƒ…å ±ã¯ã‚ˆã‚Šä¿¡é ¼ã§ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™

å›ç­”ã§ã¯ã€ä½¿ç”¨ã—ãŸæƒ…å ±ã®ä¿¡é ¼æ€§ã«ã¤ã„ã¦è¨€åŠã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæƒ…å ±ã®è³ªã‚’åˆ¤æ–­ã§ãã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚
ã¾ãŸã€æƒ…å ±æºã‚’å¼•ç”¨ã™ã‚‹éš›ã¯[1]ã€[2]ã®ã‚ˆã†ã«ç•ªå·ã‚’ä»˜ã‘ã¦ã€ã©ã®æƒ…å ±ãŒã©ã®æƒ…å ±æºã‹ã‚‰å¾—ã‚‰ã‚ŒãŸã‹ã‚’æ˜ç¢ºã«ã—ã¦ãã ã•ã„ã€‚
å›ç­”ã®æœ€å¾Œã«ã€Œå¼•ç”¨æ–‡çŒ®ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨­ã‘ã€ä½¿ç”¨ã—ãŸæƒ…å ±æºã®ã‚¿ã‚¤ãƒˆãƒ«ã€URLã€å…¬é–‹æ—¥æ™‚ã‚’åˆ—æŒ™ã—ã¦ãã ã•ã„ã€‚
"""

        if "search_enabled" not in self.config:
            self.config["search_enabled"] = True

        if "max_search_results" not in self.config:
            self.config["max_search_results"] = 3

        if "search_region" not in self.config:
            self.config["search_region"] = "jp-ja"

        if "news_search" not in self.config:
            self.config["news_search"] = True

        if "max_query_refinements" not in self.config:
            self.config["max_query_refinements"] = 1

        if "use_structured_output" not in self.config:
            self.config["use_structured_output"] = True

        if "citation_format" not in self.config:
            self.config[
                "citation_format"
            ] = "numbered"  # 'numbered', 'footnote', 'inline'ã®ã„ãšã‚Œã‹

        if "include_citations_section" not in self.config:
            self.config["include_citations_section"] = True

        self.ddgs = DDGS()
        self.state["last_search_query"] = None
        self.state["last_search_results"] = None
        self.state["refined_query"] = None

    def _ask_llm(self, prompt: str, temperature: float = 0.0) -> str:
        """
        Azure OpenAI APIã‚’ä½¿ç”¨ã—ã¦å˜ä¸€ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã™ã‚‹å›ç­”ã‚’å–å¾—ã—ã¾ã™ã€‚

        Args:
            prompt: è³ªå•ã‚„ã‚¿ã‚¹ã‚¯ã‚’å«ã‚€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            temperature: ç”Ÿæˆã®å¤šæ§˜æ€§ï¼ˆ0.0ã¯æ±ºå®šè«–çš„ã€1.0ã¯å‰µé€ çš„ï¼‰

        Returns:
            LLMã‹ã‚‰ã®å›ç­”ãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not self.client:
            raise ValueError(
                "Azure OpenAI client is not initialized. Please check your configuration."
            )

        try:
            messages = [{"role": "user", "content": prompt}]
            response = self.client.chat.completions.create(
                model=self.config["deployment_name"],
                messages=messages,
                temperature=temperature,
                stream=False,
            )

            return response.choices[0].message.content
        except Exception as e:
            print(f"Error calling Azure OpenAI API: {str(e)}")
            return ""

    def _ask_llm_with_structured_output(
        self, prompt: str, json_schema: dict, temperature: float = 0.0
    ) -> dict:
        """
        Azure OpenAI APIã‚’ä½¿ç”¨ã—ã¦æ§‹é€ åŒ–å‡ºåŠ›ã‚’è¿”ã™ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã™ã‚‹å›ç­”ã‚’å–å¾—ã—ã¾ã™ã€‚
        æ§‹é€ åŒ–å‡ºåŠ›ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯ã€é€šå¸¸ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã‚¹ã‚­ãƒ¼ãƒæƒ…å ±ã‚’å«ã‚ã¦å®Ÿè¡Œã—ã¾ã™ã€‚

        Args:
            prompt: è³ªå•ã‚„ã‚¿ã‚¹ã‚¯ã‚’å«ã‚€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            json_schema: è¿”ã•ã‚Œã‚‹æ§‹é€ åŒ–JSONã®ã‚¹ã‚­ãƒ¼ãƒ
            temperature: ç”Ÿæˆã®å¤šæ§˜æ€§ï¼ˆ0.0ã¯æ±ºå®šè«–çš„ã€1.0ã¯å‰µé€ çš„ï¼‰

        Returns:
            æ§‹é€ åŒ–ã•ã‚ŒãŸJSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆè¾æ›¸å½¢å¼ï¼‰
        """
        if not self.client:
            raise ValueError(
                "Azure OpenAI client is not initialized. Please check your configuration."
            )

        if not self.config.get("use_structured_output", True):
            return self._ask_llm_with_structured_output_fallback(
                prompt, json_schema, temperature
            )

        try:
            messages = [{"role": "user", "content": prompt}]
            response = self.client.chat.completions.create(
                model=self.config["deployment_name"],
                messages=messages,
                temperature=temperature,
                response_format={"type": "json_object", "schema": json_schema},
                stream=False,
            )

            return json.loads(response.choices[0].message.content)
        except Exception as e:
            error_str = str(e)
            if (
                "response_format.schema" in error_str
                and "unknown_parameter" in error_str
            ):
                print(
                    f"ãƒ¢ãƒ‡ãƒ« {self.config['deployment_name']} ã¯æ§‹é€ åŒ–å‡ºåŠ›ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ–¹å¼ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"
                )
                self.config["use_structured_output"] = False
                return self._ask_llm_with_structured_output_fallback(
                    prompt, json_schema, temperature
                )

            print(f"Error calling Azure OpenAI API with structured output: {error_str}")
            return {}

    def _ask_llm_with_structured_output_fallback(
        self, prompt: str, json_schema: dict, temperature: float = 0.0
    ) -> dict:
        """
        æ§‹é€ åŒ–å‡ºåŠ›ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„ãƒ¢ãƒ‡ãƒ«ç”¨ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…ã€‚
        ã‚¹ã‚­ãƒ¼ãƒã®æƒ…å ±ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸ‹ã‚è¾¼ã¿ã€JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã®å›ç­”ã‚’è¦æ±‚ã—ã¾ã™ã€‚

        Args:
            prompt: å…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            json_schema: JSONã‚¹ã‚­ãƒ¼ãƒ
            temperature: ç”Ÿæˆã®å¤šæ§˜æ€§

        Returns:
            ãƒ‘ãƒ¼ã‚¹ã•ã‚ŒãŸJSONï¼ˆè¾æ›¸å½¢å¼ï¼‰
        """
        properties = json_schema.get("properties", {})
        required = json_schema.get("required", [])

        schema_prompt = "ä»¥ä¸‹ã®JSONã‚¹ã‚­ãƒ¼ãƒã«å¾“ã£ã¦å›ç­”ã—ã¦ãã ã•ã„ï¼š\n"
        schema_prompt += "{\n"
        for prop_name, prop_info in properties.items():
            schema_prompt += f'  "{prop_name}": {{\n'
            schema_prompt += f"    \"type\": \"{prop_info.get('type', 'string')}\",\n"
            schema_prompt += (
                f"    \"description\": \"{prop_info.get('description', '')}\"\n"
            )
            schema_prompt += "  },\n"
        schema_prompt += "}\n\n"

        if required:
            schema_prompt += f"å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰: {', '.join(required)}\n\n"

        schema_prompt += "å¿…ãšæœ‰åŠ¹ãªJSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚"

        enhanced_prompt = f"{prompt}\n\n{schema_prompt}"

        response_text = self._ask_llm(enhanced_prompt, temperature)

        try:
            json_str = self._extract_json_from_text(response_text)
            parsed_json = json.loads(json_str)
            return parsed_json
        except json.JSONDecodeError as e:
            print(f"JSONè§£æã‚¨ãƒ©ãƒ¼: {e}, å›ç­”: {response_text[:100]}...")
            return {}

    def _extract_json_from_text(self, text: str) -> str:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰JSONéƒ¨åˆ†ã‚’æŠ½å‡ºã—ã¾ã™ã€‚

        Args:
            text: è§£æã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            æŠ½å‡ºã•ã‚ŒãŸJSONæ–‡å­—åˆ—
        """
        start_idx = text.find("{")
        if start_idx == -1:
            return "{}"

        open_braces = 0
        for i in range(start_idx, len(text)):
            if text[i] == "{":
                open_braces += 1
            elif text[i] == "}":
                open_braces -= 1
                if open_braces == 0:
                    return text[start_idx : i + 1]

        return "{}"  # æœ‰åŠ¹ãªJSONãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ

    def should_search(self, message: str) -> bool:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å¯¾ã—ã¦æ¤œç´¢ã‚’è¡Œã†ã¹ãã‹ã©ã†ã‹ã‚’åˆ¤æ–­ã—ã¾ã™ã€‚

        Args:
            message: åˆ†æã™ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

        Returns:
            æ¤œç´¢ã‚’è¡Œã†ã¹ãå ´åˆã¯Trueã€ãã†ã§ãªã„å ´åˆã¯False
        """
        if not self.config.get("search_enabled", True):
            return False

        if message.strip().lower().startswith(("æ¤œç´¢:", "search:")):
            return True

        prompt = f"""
        ã‚ãªãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’åˆ†æã—ã€å¤–éƒ¨æƒ…å ±ã®æ¤œç´¢ãŒå¿…è¦ã‹ã©ã†ã‹ã‚’åˆ¤æ–­ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
        
        ä»¥ä¸‹ã®ã‚ˆã†ãªå ´åˆã€å¤–éƒ¨æ¤œç´¢ãŒå¿…è¦ã¨åˆ¤æ–­ã—ã¦ãã ã•ã„ï¼š
        - æœ€æ–°ã®æƒ…å ±ã‚„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ±‚ã‚ã¦ã„ã‚‹è³ªå•
        - ç‰¹å®šã®äº‹å®Ÿã‚„çµ±è¨ˆã«é–¢ã™ã‚‹è³ªå•
        - ç‰¹å®šã®å ´æ‰€ã€äººç‰©ã€ã‚¤ãƒ™ãƒ³ãƒˆãªã©ã«é–¢ã™ã‚‹å…·ä½“çš„ãªæƒ…å ±ã‚’æ±‚ã‚ã‚‹è³ªå•
        - ã€Œèª¿ã¹ã¦ã€ã€Œæ•™ãˆã¦ã€ãªã©ã®æ˜ç¤ºçš„ãªæƒ…å ±è¦æ±‚ã‚’å«ã‚€è³ªå•
        
        ä»¥ä¸‹ã®ã‚ˆã†ãªå ´åˆã¯å¤–éƒ¨æ¤œç´¢ãŒä¸è¦ã§ã™ï¼š
        - ä¸€èˆ¬çš„ãªæ¦‚å¿µã‚„å®šç¾©ã«é–¢ã™ã‚‹è³ªå•
        - å€‹äººçš„ãªæ„è¦‹ã‚„ææ¡ˆã‚’æ±‚ã‚ã‚‹è³ªå•
        - ä¼šè©±ã®ç¶™ç¶šã‚„æŒ¨æ‹¶
        
        ã€Œ{message}ã€
        
        ã“ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯å¤–éƒ¨æƒ…å ±ã®æ¤œç´¢ãŒå¿…è¦ã‹ã©ã†ã‹ã‚’åˆ¤æ–­ã—ã€ç†ç”±ã‚‚èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
        """

        schema = SearchDecision.schema()
        result = self._ask_llm_with_structured_output(prompt, schema)

        if not result:
            return False

        return result.get("should_search", False)

    def generate_search_query(self, message: str) -> str:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¾ã™ã€‚

        Args:
            message: ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

        Returns:
            æ¤œç´¢ã‚¯ã‚¨ãƒªæ–‡å­—åˆ—
        """
        if message.strip().lower().startswith(("æ¤œç´¢:", "search:")):
            return message.split(":", 1)[1].strip()

        prompt = f"""
        ã‚ãªãŸã¯æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã®å°‚é–€å®¶ã§ã€åŠ¹æœçš„ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ä½œæˆã™ã‚‹ã‚¹ã‚­ãƒ«ã‚’æŒã£ã¦ã„ã¾ã™ã€‚
        
        ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’åˆ†æã—ã€æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„æƒ…å ±ã‚’è¦‹ã¤ã‘ã‚‹ãŸã‚ã®æœ€é©ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
        
        - ç°¡æ½”ã§å…·ä½“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹
        - ä¸è¦ãªè¨€è‘‰ï¼ˆã€Œã«ã¤ã„ã¦æ•™ãˆã¦ã€ãªã©ï¼‰ã¯çœç•¥ã™ã‚‹
        - é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã ã‘ã‚’å«ã‚ã‚‹
        - æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã§æœ€è‰¯ã®çµæœã‚’å¾—ã‚‰ã‚Œã‚‹ã‚ˆã†ã«æœ€é©åŒ–ã™ã‚‹
        - æ—¥æœ¬èªã®ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã™ã‚‹
        - ä¸»è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚‚å€‹åˆ¥ã«ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ãã ã•ã„
        
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: ã€Œ{message}ã€
        """

        schema = SearchQuery.schema()
        result = self._ask_llm_with_structured_output(prompt, schema)

        if not result or "query" not in result:
            return message.strip()

        query = result.get("query", "").strip()

        if len(query) > 100:
            query = query[:100]

        return query.strip()

    def perform_search(self, query: str) -> List[Dict[str, str]]:
        """
        Perform a DuckDuckGo search with the given query.

        Args:
            query: The search query.

        Returns:
            A list of search results.
        """
        try:
            max_results = self.config.get("max_search_results", 3)
            region = self.config.get("search_region", "jp-ja")
            use_news = self.config.get("news_search", True)

            results = []

            if use_news:
                news_results = list(
                    self.ddgs.news(
                        query,
                        region=region,
                        max_results=max(1, max_results // 2),  # æ¤œç´¢çµæœã®åŠåˆ†ã‚’ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰
                    )
                )
                results.extend(news_results)

            text_results = list(
                self.ddgs.text(
                    query,
                    region=region,
                    max_results=max_results - len(results),  # æ®‹ã‚Šã®æ ã‚’é€šå¸¸æ¤œç´¢ã§åŸ‹ã‚ã‚‹
                )
            )
            results.extend(text_results)

            formatted_results = []
            for result in results:
                date_info = self.extract_date_info(result)
                source_type = self.classify_information_source(result)

                formatted_results.append(
                    {
                        "title": result.get("title", ""),
                        "body": result.get("body", ""),
                        "href": result.get("href", ""),
                        "date": date_info,
                        "source_type": source_type,
                    }
                )

            self.state["last_search_query"] = query
            self.state["last_search_results"] = formatted_results

            return formatted_results
        except Exception as e:
            print(f"Search error: {str(e)}")
            return []

    def refine_search_query(
        self, original_query: str, results: List[Dict[str, str]]
    ) -> Optional[str]:
        """
        åˆæœŸæ¤œç´¢çµæœã«åŸºã¥ã„ã¦æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æœ€é©åŒ–ã—ã¾ã™ã€‚

        Args:
            original_query: å…ƒã®æ¤œç´¢ã‚¯ã‚¨ãƒª
            results: åˆæœŸæ¤œç´¢çµæœ

        Returns:
            æœ€é©åŒ–ãŒå¿…è¦ãªå ´åˆã¯æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã€ãã†ã§ãªã„å ´åˆã¯None
        """
        if not results or len(results) >= self.config.get("max_search_results", 3):
            return None  # ååˆ†ãªçµæœãŒã‚ã‚‹å ´åˆã¯æœ€é©åŒ–ä¸è¦

        results_summary = "\n".join(
            [
                f"ã‚¿ã‚¤ãƒˆãƒ«: {result.get('title', '')}\nå†…å®¹: {result.get('body', '')[:100]}..."
                for result in results
            ]
        )

        prompt = f"""
        ã‚ãªãŸã¯æ¤œç´¢ã‚¯ã‚¨ãƒªæœ€é©åŒ–ã®å°‚é–€å®¶ã§ã™ã€‚æ¤œç´¢çµæœãŒå°‘ãªã„å ´åˆã«ã€ã‚ˆã‚Šè‰¯ã„çµæœã‚’å¾—ã‚‹ãŸã‚ã«ã‚¯ã‚¨ãƒªã‚’æ”¹å–„ã—ã¾ã™ã€‚
        
        å…ƒã®æ¤œç´¢ã‚¯ã‚¨ãƒª: ã€Œ{original_query}ã€
        æ¤œç´¢çµæœæ•°: {len(results)}ä»¶ï¼ˆæœ€å¤§{self.config.get("max_search_results", 3)}ä»¶ä¸­ï¼‰
        
        {results_summary}
        
        ã‚ˆã‚Šå¤šãã®é–¢é€£æƒ…å ±ã‚’å¾—ã‚‹ãŸã‚ã«ã€æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ”¹å–„ã™ã¹ãã‹ã©ã†ã‹åˆ¤æ–­ã—ã€æ”¹å–„ãŒå¿…è¦ãªå ´åˆã¯æ–°ã—ã„ã‚¯ã‚¨ãƒªã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚
        ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã®æ–¹æ³•ã‚’é©ç”¨ã§ãã¾ã™ï¼š
        - ã‚ˆã‚Šä¸€èˆ¬çš„ãªç”¨èªã‚’ä½¿ç”¨ã™ã‚‹
        - åˆ¥ã®è¨€ã„å›ã—ã‚’è©¦ã™
        - é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¿½åŠ ã™ã‚‹
        - åˆ¶é™çš„ã™ãã‚‹ä¿®é£¾èªã‚’å‰Šé™¤ã™ã‚‹
        
        ã‚¯ã‚¨ãƒªã‚’å¤‰æ›´ã™ã‚‹å¿…è¦ãŒãªã„å ´åˆã¯ã€should_refineã‚’falseã«ã—ã¦ãã ã•ã„ã€‚
        """

        schema = QueryRefinement.schema()
        result = self._ask_llm_with_structured_output(prompt, schema)

        if not result:
            return None

        should_refine = result.get("should_refine", False)
        refined_query = result.get("refined_query", "").strip()

        if should_refine and refined_query and refined_query != original_query:
            return refined_query

        return None

    def extract_date_info(self, result: Dict[str, str]) -> Optional[str]:
        """
        æ¤œç´¢çµæœã‹ã‚‰å…¬é–‹æ—¥æƒ…å ±ã‚’æŠ½å‡ºã—ã¾ã™ã€‚

        Args:
            result: æ—¥ä»˜ã‚’æŠ½å‡ºã™ã‚‹æ¤œç´¢çµæœ

        Returns:
            æ—¥ä»˜ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã¯ãã®æ–‡å­—åˆ—è¡¨ç¾ã€è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆã¯None
        """
        if "published" in result:
            return result.get("published", None)

        title = result.get("title", "")
        body = result.get("body", "")

        prompt = f"""
        ã‚ãªãŸã¯ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ—¥ä»˜æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚
        
        ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å…¬é–‹æ—¥ã¾ãŸã¯æœ€å¾Œã®æ›´æ–°æ—¥ã¨æ€ã‚ã‚Œã‚‹æ—¥ä»˜æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
        
        ã‚¿ã‚¤ãƒˆãƒ«: {title}
        æœ¬æ–‡: {body}
        
        - æ—¥ä»˜ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€date_foundã‚’falseã«ã—ã¦ãã ã•ã„
        - è¤‡æ•°ã®æ—¥ä»˜ãŒã‚ã‚‹å ´åˆã¯ã€æœ€ã‚‚æœ€è¿‘ã®ã‚‚ã®ã‚’é¸æŠã—ã¦ãã ã•ã„
        - æ—¥ä»˜ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’æŒ‡å®šã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼šYYYYå¹´MMæœˆDDæ—¥ã€YYYY/MM/DD ãªã©ï¼‰
        """

        schema = DateExtraction.schema()
        result = self._ask_llm_with_structured_output(prompt, schema)

        if not result or not result.get("date_found", False):
            return None

        return result.get("date", None)

    def classify_information_source(self, result: Dict[str, str]) -> str:
        """
        æ¤œç´¢çµæœã‚’ä¸€æ¬¡æƒ…å ±ã¾ãŸã¯äºŒæ¬¡æƒ…å ±ã¨ã—ã¦åˆ†é¡ã—ã¾ã™ã€‚

        Args:
            result: åˆ†é¡ã™ã‚‹æ¤œç´¢çµæœ

        Returns:
            "ä¸€æ¬¡æƒ…å ±"ã€"äºŒæ¬¡æƒ…å ±"ã€ã¾ãŸã¯"ä¸æ˜"ã¨ã—ã¦ã®åˆ†é¡
        """
        href = result.get("href", "")
        title = result.get("title", "")
        body = result.get("body", "")

        prompt = f"""
        ã‚ãªãŸã¯æƒ…å ±ã‚½ãƒ¼ã‚¹ã®åˆ†é¡å°‚é–€å®¶ã§ã™ã€‚æƒ…å ±ã‚’ä¸€æ¬¡æƒ…å ±ï¼ˆç›´æ¥ã®æƒ…å ±æºï¼‰ã¨äºŒæ¬¡æƒ…å ±ï¼ˆé–“æ¥çš„ãªæƒ…å ±æºï¼‰ã«åˆ†é¡ã—ã¾ã™ã€‚
        
        ä¸€æ¬¡æƒ…å ±ï¼ˆãƒ—ãƒ©ã‚¤ãƒãƒªãƒ¼ã‚½ãƒ¼ã‚¹ï¼‰:
        - æ”¿åºœã‚„å…¬å¼æ©Ÿé–¢ã‹ã‚‰ã®ç›´æ¥ã®ç™ºè¡¨
        - ä¼æ¥­ã‚„çµ„ç¹”ã®å…¬å¼ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‚„ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹
        - ç›´æ¥ã®è¨¼è¨€ã‚„ä¸€æ¬¡è³‡æ–™
        - åŸè‘—è«–æ–‡ã‚„ç ”ç©¶å ±å‘Šæ›¸
        
        äºŒæ¬¡æƒ…å ±ï¼ˆã‚»ã‚«ãƒ³ãƒ€ãƒªãƒ¼ã‚½ãƒ¼ã‚¹ï¼‰:
        - ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã‚„å ±é“æ©Ÿé–¢ã«ã‚ˆã‚‹å ±é“
        - ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚„ãƒ¬ãƒ“ãƒ¥ãƒ¼
        - è§£èª¬è¨˜äº‹ã‚„åˆ†æ
        - ã¾ã¨ã‚ã‚µã‚¤ãƒˆã‚„æƒ…å ±ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        
        URL: {href}
        ã‚¿ã‚¤ãƒˆãƒ«: {title}
        å†…å®¹: {body[:300]}...
        
        ä¸Šè¨˜ã®æƒ…å ±ã‚½ãƒ¼ã‚¹ã‚’åˆ†æã—ã€ã€Œä¸€æ¬¡æƒ…å ±ã€ã€ã€ŒäºŒæ¬¡æƒ…å ±ã€ã€ã¾ãŸã¯ã€Œä¸æ˜ã€ã«åˆ†é¡ã—ã¦ãã ã•ã„ã€‚
        åˆ†é¡ã®ç¢ºä¿¡åº¦ï¼ˆ0.0-1.0ï¼‰ã¨ç†ç”±ã‚‚èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
        """

        schema = SourceClassification.schema()
        result = self._ask_llm_with_structured_output(prompt, schema)

        if not result:
            return "ä¸æ˜"

        source_type = result.get("source_type", "ä¸æ˜")

        return source_type

    def format_search_results(self, results: List[Dict[str, str]]) -> str:
        """
        Format search results as a string.

        Args:
            results: The search results to format.

        Returns:
            Formatted search results string.
        """
        if not results:
            return "æ¤œç´¢çµæœã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"

        formatted = "æ¤œç´¢çµæœ:\n\n"
        for i, result in enumerate(results, 1):
            formatted += f"{i}. {result['title']}\n"
            formatted += f"   {result['body']}\n"

            if result.get("date"):
                formatted += f"   ğŸ“… å…¬é–‹æ—¥: {result['date']}\n"

            source_type = result.get("source_type", "ä¸æ˜")
            formatted += f"   ğŸ“Š æƒ…å ±ã®ç¨®é¡: {source_type}\n"

            formatted += f"   ğŸ”— å‡ºå…¸: {result['href']}\n\n"

        citations = self.generate_citations(results)
        formatted += "\nå¼•ç”¨å½¢å¼:\n"
        for citation in citations:
            formatted += f"[{citation.source_id}] {citation.title}. "
            if citation.date:
                formatted += f"({citation.date}). "
            formatted += f"{citation.url}\n"

        return formatted

    def generate_citations(
        self, search_results: List[Dict[str, str]]
    ) -> List[SourceCitation]:
        """
        æ¤œç´¢çµæœã‹ã‚‰å¼•ç”¨æƒ…å ±ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

        Args:
            search_results: æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ

        Returns:
            å¼•ç”¨æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        citations = []

        for i, result in enumerate(search_results, 1):
            citation = SourceCitation(
                source_id=i,
                title=result.get("title", ""),
                url=result.get("href", ""),
                date=result.get("date", None),
                source_type=result.get("source_type", "ä¸æ˜"),
                excerpt=result.get("body", "")[:150] + "..."
                if len(result.get("body", "")) > 150
                else result.get("body", ""),
            )
            citations.append(citation)

        return citations

    def format_citation_instructions(self, citations: List[SourceCitation]) -> str:
        """
        å¼•ç”¨æƒ…å ±ã‹ã‚‰å¼•ç”¨æŒ‡ç¤ºæ–‡ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

        Args:
            citations: å¼•ç”¨æƒ…å ±ã®ãƒªã‚¹ãƒˆ

        Returns:
            å¼•ç”¨æŒ‡ç¤ºæ–‡
        """
        citation_instruction = """
æ¤œç´¢çµæœã®æƒ…å ±ã‚’å¼•ç”¨ã™ã‚‹å ´åˆã¯ã€[1]ã€[2]ã®ã‚ˆã†ã«ã‚½ãƒ¼ã‚¹ç•ªå·ã‚’æ–‡ä¸­ã«å«ã‚ã¦ãã ã•ã„ã€‚
å›ç­”ã®æœ€å¾Œã«ã€Œå¼•ç”¨æ–‡çŒ®ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨­ã‘ã€ä»¥ä¸‹ã®å½¢å¼ã§æƒ…å ±æºã‚’åˆ—æŒ™ã—ã¦ãã ã•ã„:

å¼•ç”¨æ–‡çŒ®:
"""
        for citation in citations:
            citation_instruction += f"[{citation.source_id}] {citation.title}"
            if citation.date:
                citation_instruction += f" ({citation.date})"
            citation_instruction += f". {citation.url}\n"

        return citation_instruction

    def process_message(
        self, message: str, context: Optional[List[Dict[str, str]]] = None
    ) -> Generator[str, None, Dict[str, Any]]:
        """
        Process a user message and generate a streaming response with search integration.

        Args:
            message: The user message to process.
            context: Optional conversation history or context.

        Returns:
            A generator that yields response chunks for streaming and finally returns
            a dictionary containing the complete response and any additional metadata.
        """
        if not self.client:
            raise ValueError(
                "Azure OpenAI client is not initialized. Please check your configuration."
            )

        if message.lower().startswith(("source:", "ã‚½ãƒ¼ã‚¹:", "å‡ºå…¸:", "å¼•ç”¨:")):
            source_generator = self.handle_source_command(message)
            response_dict = None

            for chunk in source_generator:
                yield chunk

            try:
                response_dict = source_generator.send(None)
            except StopIteration as e:
                if hasattr(e, "value") and isinstance(e.value, dict):
                    response_dict = e.value
                else:
                    response_dict = {
                        "response": "ã‚½ãƒ¼ã‚¹æƒ…å ±ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚",
                        "error": "ç„¡åŠ¹ãªå¿œç­”å½¢å¼",
                        "timestamp": time.time(),
                    }

            if not isinstance(response_dict, dict):
                response_dict = {
                    "response": str(response_dict)
                    if response_dict
                    else "ã‚½ãƒ¼ã‚¹æƒ…å ±ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚",
                    "error": "ç„¡åŠ¹ãªå¿œç­”å½¢å¼",
                    "timestamp": time.time(),
                }

            return response_dict

        search_results = []
        search_info = ""

        if self.should_search(message):
            query = self.generate_search_query(message)

            yield f"<search_start>ğŸ” ã€Œ{query}ã€ã‚’æ¤œç´¢ä¸­...</search_start>"
            search_results = self.perform_search(query)

            if (
                len(search_results) < 2
                and self.config.get("max_query_refinements", 1) > 0
            ):
                refined_query = self.refine_search_query(query, search_results)
                if refined_query and refined_query != query:
                    yield f"<search_start>ğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ã€Œ{refined_query}ã€ã«æ”¹å–„ã—ã¦å†æ¤œç´¢ä¸­...</search_start>"
                    self.state["refined_query"] = refined_query
                    search_results = self.perform_search(refined_query)

            yield "<search_end>"

            if search_results:
                search_info = self.format_search_results(search_results)
                yield f"{search_info}\n\nå›ç­”ã‚’ç”Ÿæˆä¸­...\n\n"

        messages = []

        system_message = self.config.get(
            "system_message", "ã‚ãªãŸã¯å¤–éƒ¨æƒ…å ±æ¤œç´¢æ©Ÿèƒ½ã‚’æŒã¤å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
        )

        if search_results:
            citations = self.generate_citations(search_results)
            self.state["citations"] = citations

            citation_instruction = self.format_citation_instructions(citations)

            system_message += f"\n\nä»¥ä¸‹ã®æ¤œç´¢çµæœã‚’å‚è€ƒã«ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚æ¤œç´¢çµæœãŒè³ªå•ã«é–¢é€£ã—ãªã„å ´åˆã¯ç„¡è¦–ã—ã¦ãã ã•ã„ã€‚\n\n{search_info}\n\n{citation_instruction}"

        messages.append({"role": "system", "content": system_message})

        if context:
            messages.extend(context)

        messages.append({"role": "user", "content": message})

        try:
            stream = self.client.chat.completions.create(
                model=self.config["deployment_name"],
                messages=messages,
                stream=True,
            )

            full_response = ""

            for chunk in stream:
                if (
                    chunk.choices
                    and len(chunk.choices) > 0
                    and chunk.choices[0].delta
                    and chunk.choices[0].delta.content is not None
                ):
                    content = chunk.choices[0].delta.content
                    full_response += content
                    yield content

            return {
                "response": full_response,
                "model": self.config["deployment_name"],
                "timestamp": time.time(),
                "search_performed": bool(search_results),
                "search_query": self.state.get("last_search_query"),
                "search_results": self.state.get("last_search_results"),
                "structured_output_supported": self.config.get(
                    "use_structured_output", True
                ),
            }

        except Exception as e:
            error_message = f"Error calling Azure OpenAI API: {str(e)}"
            yield error_message
            return {
                "response": error_message,
                "error": str(e),
                "timestamp": time.time(),
            }

    def get_response(
        self, message: str, context: Optional[List[Dict[str, str]]] = None
    ) -> Dict[str, Any]:
        """
        Process a user message and return a complete response (non-streaming) with search integration.

        Args:
            message: The user message to process.
            context: Optional conversation history or context.

        Returns:
            A dictionary containing the complete response and any additional metadata.
        """
        if not self.client:
            raise ValueError(
                "Azure OpenAI client is not initialized. Please check your configuration."
            )

        search_results = []
        search_info = ""

        if self.should_search(message):
            query = self.generate_search_query(message)

            search_results = self.perform_search(query)

            if search_results:
                search_info = self.format_search_results(search_results)

        messages = []

        system_message = self.config.get(
            "system_message", "ã‚ãªãŸã¯å¤–éƒ¨æƒ…å ±æ¤œç´¢æ©Ÿèƒ½ã‚’æŒã¤å„ªç§€ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
        )

        if search_results:
            citations = self.generate_citations(search_results)
            self.state["citations"] = citations

            citation_instruction = self.format_citation_instructions(citations)

            system_message += f"\n\nä»¥ä¸‹ã®æ¤œç´¢çµæœã‚’å‚è€ƒã«ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚æ¤œç´¢çµæœãŒè³ªå•ã«é–¢é€£ã—ãªã„å ´åˆã¯ç„¡è¦–ã—ã¦ãã ã•ã„ã€‚\n\n{search_info}\n\n{citation_instruction}"

        messages.append({"role": "system", "content": system_message})

        if context:
            messages.extend(context)

        messages.append({"role": "user", "content": message})

        try:
            response = self.client.chat.completions.create(
                model=self.config["deployment_name"],
                messages=messages,
                stream=False,
            )

            content = response.choices[0].message.content

            return {
                "response": content,
                "model": self.config["deployment_name"],
                "timestamp": time.time(),
                "finish_reason": response.choices[0].finish_reason,
                "search_performed": bool(search_results),
                "search_query": self.state.get("last_search_query"),
                "search_results": self.state.get("last_search_results"),
                "structured_output_supported": self.config.get(
                    "use_structured_output", True
                ),
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens,
                },
            }

        except Exception as e:
            error_message = f"Error calling Azure OpenAI API: {str(e)}"
            return {
                "response": error_message,
                "error": str(e),
                "timestamp": time.time(),
            }

    def extract_source_content(self, source_id: int) -> Optional[Dict[str, Any]]:
        """
        æŒ‡å®šã•ã‚ŒãŸã‚½ãƒ¼ã‚¹IDã®æƒ…å ±ã‚’æŠ½å‡ºãƒ»å±•é–‹ã—ã¾ã™ã€‚

        Args:
            source_id: æŠ½å‡ºã™ã‚‹ã‚½ãƒ¼ã‚¹ã®ID

        Returns:
            ã‚½ãƒ¼ã‚¹ã®è©³ç´°æƒ…å ±ã‚’å«ã‚€è¾æ›¸ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯None
        """
        if not self.state.get("last_search_results"):
            return None

        search_results = self.state.get("last_search_results")

        if not search_results or source_id <= 0 or source_id > len(search_results):
            return None

        result = search_results[source_id - 1]

        extracted_info = {
            "title": result.get("title", ""),
            "full_content": result.get("body", ""),
            "url": result.get("href", ""),
            "date": result.get("date", None),
            "source_type": result.get("source_type", "ä¸æ˜"),
            "source_id": source_id,
        }

        prompt = f"""
        ä»¥ä¸‹ã®æƒ…å ±æºã‹ã‚‰ä¸»è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’æŠ½å‡ºã—ã€å†…å®¹ã‚’è¦ç´„ã—ã¦ãã ã•ã„ï¼š
        
        ã‚¿ã‚¤ãƒˆãƒ«: {extracted_info["title"]}
        å†…å®¹: {extracted_info["full_content"]}
        """

        try:
            key_points_analysis = self._ask_llm(prompt)
            extracted_info["key_points"] = key_points_analysis
        except Exception as e:
            extracted_info["key_points"] = "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"

        return extracted_info

    def handle_source_command(
        self, message: str
    ) -> Generator[str, None, Dict[str, Any]]:
        """
        ã‚½ãƒ¼ã‚¹å±•é–‹ã‚³ãƒãƒ³ãƒ‰ã‚’å‡¦ç†ã—ã¾ã™ã€‚

        Args:
            message: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ (ä¾‹: "source:1" or "ã‚½ãƒ¼ã‚¹:2")

        Returns:
            å±•é–‹ã•ã‚ŒãŸã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’å«ã‚€ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼
        """
        try:
            if message.lower().startswith(("source:", "ã‚½ãƒ¼ã‚¹:", "å‡ºå…¸:", "å¼•ç”¨:")):
                source_id_str = message.split(":", 1)[1].strip()
                try:
                    source_id = int(source_id_str)
                except ValueError:
                    yield f"ã‚¨ãƒ©ãƒ¼: æœ‰åŠ¹ãªã‚½ãƒ¼ã‚¹IDã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚ä¾‹: ã‚½ãƒ¼ã‚¹:1"
                    return {
                        "response": "ã‚¨ãƒ©ãƒ¼: æœ‰åŠ¹ãªã‚½ãƒ¼ã‚¹IDã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚",
                        "error": "ç„¡åŠ¹ãªã‚½ãƒ¼ã‚¹ID",
                        "timestamp": time.time(),
                    }

                source_info = self.extract_source_content(source_id)

                if not source_info:
                    yield f"ã‚¨ãƒ©ãƒ¼: ã‚½ãƒ¼ã‚¹ID {source_id} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
                    return {
                        "response": f"ã‚¨ãƒ©ãƒ¼: ã‚½ãƒ¼ã‚¹ID {source_id} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚",
                        "error": "ã‚½ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“",
                        "timestamp": time.time(),
                    }

                response = f"# ã‚½ãƒ¼ã‚¹ {source_id}: {source_info['title']}\n\n"
                response += f"**URL**: {source_info['url']}\n"
                if source_info.get("date"):
                    response += f"**å…¬é–‹æ—¥**: {source_info['date']}\n"
                response += f"**æƒ…å ±ã®ç¨®é¡**: {source_info['source_type']}\n\n"
                response += f"## å†…å®¹\n{source_info['full_content']}\n\n"
                response += f"## ä¸»è¦ãƒã‚¤ãƒ³ãƒˆ\n{source_info['key_points']}\n"

                yield response

                return {
                    "response": response,
                    "source_info": source_info,
                    "timestamp": time.time(),
                }

            return {
                "response": "",
                "is_source_command": False,
                "timestamp": time.time(),
            }

        except Exception as e:
            error_message = f"ã‚½ãƒ¼ã‚¹å±•é–‹ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            yield error_message
            return {
                "response": error_message,
                "error": str(e),
                "timestamp": time.time(),
            }

    def get_capabilities(self) -> List[str]:
        """
        Get a list of the agent's capabilities.

        Returns:
            A list of capability strings.
        """
        return [
            "text_generation",
            "streaming_response",
            "conversation_context",
            "external_search",
            "real_time_information",
            "source_citation",
            "source_content_extraction",
        ]
